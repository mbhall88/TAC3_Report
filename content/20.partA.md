## Part A: Progress Report {.page_break_before}

### Examining bacterial variation with genome graphs

### Executive summary

Genomics is now ubiquitous in clinical and public health microbiology, at least in the
developed world. However, many significant challenges remain.

- Bacterial genomes harbour huge amounts of diversity, even within a species, and
  traditional reference-based approaches are problematic.
- Much of the variation in bacteria is fundamentally inaccessible to short reads.
- Long Nanopore reads are noisy, and SNP calling with this data is not properly
  benchmarked or standardised.
- Since *Mycobacterium tuberculosis* (Mtb) infects so many people, there is potential
  for considerable impact for clinical applications.
- There is also much to be gained from a high-quality pan-genome of Mtb as well as a
  detailed map of its enigmatic *pe/ppe* gene repertoire.

These motivations drive the following PhD thesis structure:

1. Develop algorithms and software for variant discovery using bacterial genome graphs,
   building on work of a previous student in the lab (my first paper, second author).
2. Benchmark Nanopore versus Illumina SNP calling, showing our algorithms meet the needs
   of clinical and public health users, validate, and publish (second paper).
3. Improve upon current whole-genome sequencing-based drug resistance prediction for Mtb
   using genome graphs.
4. Curate a high-quality reference pan-genome for Mtb that includes a detailed map of
   the *pe/ppe* genes.

<!--================================================================================-->

### Motivation and Background

In 2018, 1.4 million people died of tuberculosis (TB) globally, and over 10 million
people fell ill to the disease. There were also 484,000 new cases of
rifampicin-resistant TB reported, of which 78% were multi-drug resistant (MDR)
[@isbn:9789241565714]. Drug-resistant TB is not a new problem; the first clinical trial
using Streptomycin to treat TB reported resistance and this outcome influenced the
creation of the four-drug first-line regimen used today. Standard of care requires
phenotypic testing of the infecting organism against these four drugs to ensure that
appropriate treatment is prescribed. However, *Mycobacterium tuberculosis* (Mtb), the
causative agent of TB, is a slow-growing organism and the gold-standard phenotypic tests
("mix bug and drug") take around two months to complete. Thus, the traditional clinical
diagnostic and treatment regimen is slow and expensive. Whole-genome sequencing offers a
faster solution; recently it was shown that equivalent results are achievable by
sequencing Mtb grown in "liquid culture" (also known as MGIT, Mycobacterial Growth
Indicator Tube) after two weeks of culture in contrast to the two-month traditional
(Lowenstein-Jensen) culture method [@doi:10.1128/JCM.03073-14]. A number of genes are
implicated in drug resistance and predicting resistance from sequencing data based on a
catalogue of resistance single-nucleotide polymorphisms (SNPs) and indels (insertions or
deletions) works with high-specificity [@doi:10/f755tg; @doi:10/f3jjtq]. For the four
first-line drugs, a study by the CRyPTIC consortium is the first of its kind to
demonstrate that phenotyping is not required if genotype predicts susceptibility
[@doi:10.1056/NEJMoa1800474]. However, as the genetic basis for drug resistance is not
entirely understood, there is still a sensitivity gap that differs drug-by-drug.

Public health requirements for TB diagnostics are **resistance prediction, species
identification, and clustering** of genomes. The clusters are intended to contain
potential transmission events, thus enabling more effective contact tracing. All of the
main requirements are currently successfully achieved with Illumina sequencing
technology. However, there are reasons to consider abandoning Illumina for Oxford
Nanopore Technology's (ONT) sequencer. First, there is cost - Illumina has raised its
reagent prices considerably. Second, the burden of TB lies primarily in places where
there is neither capital nor infrastructure for purchasing or maintaining a large
machine. The third is speed: Votintseva *et al.* showed that it was feasible to go from
patient to result via a Nanopore sequencer in 12.5 hours [@doi:10.1128/JCM.02483-16].
Since this publication, the yield and quality of Nanopore has risen. Therefore, a
diagnostic that delivers results while the patient is in the clinic is now imaginable.

As an aside, it is worth noting that in high prevalence areas, there is no interest in
transmission analysis, and therefore all of the information that they need (species and
resistance) could be attained via deep amplicon sequencing. We accept this but ignore it
and focus on a whole-genome solution nevertheless; both because of the better
phylogenetic resolution, and because we do not yet know the relevant genetic mutations
associated with new drugs.

The idea of using a single, linear reference genome to represent a population of
individuals is not ideal. Even more so in the context of a bacterium such as *Salmonella
enterica*, where two individuals can differ to such a degree that they only share 16% of
their genes [@doi:10/gfw8gq; @doi:10/gczkbr]. This fluidity of genetic content leads to
the concept of a pan-genome; defined as the set of all genes observed in a species. Some
genes are more common than others within this pan-genome, leading to the distinction of
the 'core' and 'accessory' genome with the core being those shared across (most) all
individuals and accessory being everything else. The effect of using a single-reference
genome to describe variation within samples from the same species is best illustrated in
Figure {@fig:refbias}. In this toy example, the maximum number of SNPs we can hope to
discover is only 56%. Clearly, comparing many samples requires better methods than
single-reference based ones.

![An illustration of how reference bias can impact variant-calling. Each vertical
column signifies an individual genome, with the coloured blocks representing genes.
Numbers label 50 segregating SNPs. The percentages at the bottom express the proportion
of SNPs which can be detected using each of the 6 'genomes' as a reference by mapping
perfect reads from the remaining 5 to this reference.](images/refbias.png){#fig:refbias
height="4in"}

One of the first questions that arise from a computational point-of-view is how to apply
current methods, which assume a single reference, to a pan-genome? An approach to
answering this question that has been gaining considerable traction in recent years is
that of genome graphs. This new paradigm uses a population reference graph (PRG) as its
equivalent of a single, linear, reference genome. A PRG is built to represent variation
seen within a population, conceding the fact that no single genome can accurately
represent an entire species. To construct such a PRG, one takes a multiple sequence
alignment (MSA) and collapses shared sequence and creates "bubbles", or branch points,
where they do not (see Figure {@fig:prg} for an illustration of this process). We define
a collection of PRGs as a pan-genome reference graph (PanRG), which we will
interchangeably refer to as a pan-genome. Thus far, genome graph methods have focused
mainly on eukaryotes. Given the rich diversity of genetic content in the prokaryote
world, it would seem genome graphs are more suited to utilisation there.

![An illustration of how a population reference graph (PRG) is constructed. Regions
(columns) of shared sequence are collapsed into a single node. Those that differ are
split into "bubbles", or branching nodes.](images/prg.png){#fig:prg width="100%"}

Nanopore sequencing yields very long reads (up to 2.2Mbp [@doi:10/gfnkxj]), the
mean/mode read identity tends to fall in the range of 87-94% (using ONT's `guppy`
basecaller), while on a consensus level, it can achieve 99.94% with assembly polishing
[@doi:10/gf4jwm].  
Variant calling with Nanopore sequencing data has seen a somewhat slow development.
Currently, the main tools that have had reasonable testing done and are versatile enough
to detect both SNPs and (some) indels are `nanopolish` [@doi:10/f88652], Clair
[@doi:10/d9kq] and `medaka` [@https://github.com/nanoporetech/medaka]. A recent
benchmark showed that Nanopore variant calling provides reliable diagnostic information
for *Neisseria gonorrhoeae* [@doi:10.1101/gr.262865.120]. However, to date, there has
been no extensive Nanopore variant calling benchmark done for Mtb. Given the potential
benefits of using genome graphs and long-read Nanopore sequencing for bacterial
genomics, it makes sense to try and blend the two.

Pandora is a method being developed in the group to genotype across the *entire*
pan-genome of a bacterial sample - not just the core. It does this by working with a
PRG, rather than a linear reference. The method is based on the following intuition
(similar to that behind the Li and Stephens model in population genetics): genomes
evolve by recombination and mutation, and thus we ought to be able to approximate a
$N+1$ genome as a mosaic of the first $N$ genomes. `pandora` maps Nanopore reads to a
graph encoding of a PRG, infers a mosaic, and provides genotypes at all variants in the
PanRG. Mapping is done using minimising k-mers [@doi:10/dkhs8w] in a similar vein to
that done by `minimap`[@doi:10/f8zxc3], and is therefore fast. By using Nanopore
sequencing data, it is also possible to infer gene order as, in general, a single read
will contain multiple genes, as opposed to Illumina sequencing where multiple reads are
required to span a single gene. Note `pandora` does not (prior to this work) include any
facility for discovering novel variation.

#### Summary

This PhD seeks to develop new methods to enable Nanopore-based diagnostics and
epidemiology for Mtb and other bacteria. By using PRGs as a strong prior [@doi:10/d9ks;
@doi:10.1038/ng.3257], we should be able to mitigate the Nanopore error biases and indel
issues. In the process, we aim to construct a high-quality reference pan-genome for Mtb
that we hope will open previously inaccessible parts of its genome for investigation.

<!--================================================================================-->

### Chapter 1: Variant discovery in genome graphs

Variation in bacterial genomes can arise through a diverse range of processes. Mutations
can occur during replication and are inherited vertically, genetic material can be
transferred horizontally, and homologous recombination can lead to eukaryote-like gene
conversion [@doi:10/b4x7zf]. This breadth of ways in which bacteria can acquire new and
varied genetic material results gives rise to the phenomenon of a pan-genome. Bacterial
species with an "open" pan-genome may have individuals in their population who can share
as little as 16% of their genes (*S. enterica*) [@doi:10/gfw8gq]. Not all species'
pan-genomes are this open, but it does raise the question: what do we use as a
"reference" genome for such a species? One solution is to use the reference genome for
the specific strain of interest. This works fine when dealing with a single sample or
multiple samples of the same strain. However, when expanding to many samples from
varying strains, the reference is no longer representative. An alternative solution is
to focus solely on the core genome - the complement of genes found in all members of a
species. The issue with this approach is the loss of information about variation in all
of the non-core genes, which could be a large number if the pan-genome is open. See
Figure {@fig:pangenome} for an illustration of this reference-bias problem.

<!--![Schematic representation of a pan-genome. Each oval represents the-->
<!--genetic content of a single individual within a species. Reference [@doi:10/gfw8gq].](images/pangenome.jpg){#fig:pangenome-->
<!--width="50%"}-->

The message we are aiming to drive home at the beginning of this chapter is that
reference-based analysis fundamentally cannot access all of the variation in a bacterial
species.

#### Prior work: Mosaic approximations and genotyping

Standard approaches to variant analysis are effectively a first-order approximation. In
such an approximation, samples are considered identical to the reference, one maps reads
to it, identifies clear SNPs via the read pileup, and then modifies the reference to get
an estimate of the sample's genome. As mentioned in
[Motivation and Background](#motivation-and-background), `pandora` is a method developed
by a previous PhD student in the lab, Rachel Colquhoun. It works on the premise of
approximating a genome as a hierarchical mosaic. At a high-level, it represents a mosaic
of loci - usually genes and intergenic regions - while at the locus-level, it is a
mosaic of previously-seen genomes.

`pandora` aims to infer a consensus sequence from a PRG for a single sample or a
collection of samples. In the case of a collection of samples, the consensus sequence
will be one that collection of samples. `pandora` can additionally perform genotyping of
the sample(s) with respect to this inferred consensus and produces a Variant Call Format
(VCF) file. If a gene is present in only 2 of 50 samples then genotyping information is
provided for those 2 samples and null for the other 48.

While `pandora`, before the work in this chapter, allows comparison of genomes to a
level of detail provided by no other tool, there is still a significant shortcoming: it
cannot discover novel variation. If a sample contains a variant not present in the PRG,
the best `pandora` can do is select the path that is closest to that variant. The work
in my first chapter outlines a method for removing this limitation within `pandora` and
provides an analysis of the gain in recall and precision by incorporating *de novo*
variant discovery into the `pandora` workflow.

#### Local *de novo* variant discovery in a genome graph

There are two significant difficulties in discovering *de novo* variants on a graph. The
first is finding regions within the graph that look like the reads mapping to them
contain variation we do not have in the PRG. Secondly, we need to generate new paths for
these regions and add them back into the PRG for consideration when remapping.

##### Finding candidate regions

We define a candidate region, $r$, as an interval within a local graph where coverage on
the maximum likelihood path is less than a given threshold, $c$, for more than $l$
consecutive positions. Any $r$ within a specified distance of each other are then
merged. For a given read that has a mapping to $r$, we define $s_r$ to be the
subsequence of the read mapping to $r$. We define the pileup $P_r$ as the set of all
$s_r \in r$.

##### Enumerating paths through candidate regions

We construct a de Bruijn graph from each $P_r$. Using anchor k-mers either side of the
candidate region, find all paths that exist in the de Bruijn graph between the anchors,
using a depth-first search (DFS) tree - starting with the left anchor. If the path is
longer than a certain threshold, we discard it. Or, if there are too many paths, we
abandon variant discovery for that region altogether.

##### Pruning the path-space in a candidate region

As the path enumeration step has the potential for combinatorial explosion due to cycles
in the graph, and erroneous reads causing "dead-ends", we prune the DFS tree. To do
this, we produce a distance map $D_r$ by running reversed breadth-first search (BFS) on
the de Bruijn graph, beginning from the *right* anchor. Each entry in the distance map
describes the shortest path from that node to the end anchor. So as we walk along the
DFS tree, if a node is not reachable within our maximum distance threshold, we abandon
path enumeration.

I programmed the above methods in C++ and added them into the code base for `pandora`.
They constitute 1325 lines of source code and 3486 lines of test code. I had help with
the implementation of multi-threading the *de novo* component and the BFS pruning from
Leandro Ishi. The overall workflow for `pandora` with *de novo* variant discovery
enabled is illustrated in Figure {@fig:pandora}.

<!--![Schematic of the `pandora` method. Note Steps 4-6 are those developed and added-->
<!--in the scope of chapter 1. 1) A Population Reference Graph (PRG) built from known-->
<!--variants or previous samples. 2) Sequencing reads to try and map on to the PRG. 3) Reads-->
<!--mapped on to the PRG (where possible) using minimising k-mers. Coloured segments show-->
<!--paths where reads map. 4) Local assembly of the reads using a de Bruijn graph due to-->
<!--lower than expected coverage on a section of the PRG (many unmapped reads). A new path-->
<!--is discovered (orange). 5) The new path is added into the original PRG. 6) Reads are-->
<!--remapped to the new PRG with many reads mapping to the newly included path (orange).](images/pandora.png){#fig:pandora-->
<!--width="80%"}-->

#### Evaluation

##### Simulated data

I have previously presented results from evaluating this new *de novo* variant discovery
method on simulated data to ensure if does indeed do as it advertises. For the sake of
keeping this report concise, I will refrain from repeating the results here, except to
say that this method does indeed find a large number of variants not already in the PRG.

<!--The first step in evaluating the effect of adding *de novo* variant calling to `pandora`-->
<!--is with a simulated dataset. What we aim to show here is that the addition of *de novo*-->
<!--discovery allows `pandora` to improve its probability of variant detection (recall). The-->
<!--first step was randomly selecting 100 gene MSAs from a pool of 29702 obtained for-->
<!--*Escherichia coli* from the panX database [@doi:10/gczkbr]. Next, a PRG is constructed-->
<!--for each gene MSA, and a random path through each is selected using `pandora`. Each-->
<!--PRG's random path is then concatenated together to form a single "genome" sequence. We-->
<!--subsequently add SNPs to the simulated genome at a specified rate of SNPs per-gene using-->
<!--`snp-mutator` [@doi:10/d9np]. We then simulated Nanopore reads from this mutated genome-->
<!--using `nanosim-h` [@doi:10/gbj69k; @doi:10/d9nq]. `pandora map` is then run, using the-->
<!--original PRG as input, along with the reads simulated from the mutated genome. With this-->
<!--approach, we know the exact SNPs we hope to find. After running the `map` routine, we-->
<!--are left with a collection of candidate paths produced by the *de novo* component. We-->
<!--then added these back into the PRG and iterated the `map` routine again, this time-->
<!--without *de novo* discovery enabled. Re-adding variants into the graph has the effect of-->
<!--making all of the *de novo* variants visible to genotyping. The result is a VCF file-->
<!--that (hopefully) contains the variants we introduced in the beginning. To compare the-->
<!--performance of `pandora` before and after *de novo* is added, we can compare the VCF-->
<!--produced from the first round of `map` with the last VCF.-->

<!--To avoid error-prone conversion of linear coordinates into graph coordinates the-->
<!--evaluation of whether the variants called by `pandora` are correct was undertaken in a-->
<!--slightly more convoluted manner. We define a probe-set $P$ as a collection of probes,-->
<!--$p$, where $p$ represents an entry, $e$, in a VCF file, $V$. For each $e \in V$, $p$ is-->
<!--constructed by the concatenation of $l_w$, $e_c$, and $r_w$ (in that order), where $e_c$-->
<!--is the called variant of $e$, and $l_w$ and $r_w$ are the sequences, of maximum length-->
<!--$w$, in the VCF reference to the left and right, respectively, of $e_c$.-->

<!--A truth probe-set, $P_t$, was constructed from the VCF of variants added to the-->
<!--simulated genome and a query probe-set, $P_q$, from the variants called by `pandora`. We-->
<!--then mapped all probes from $P_t$ to $P_q$ using `bwa mem` [@arxiv:1303.3997]. We then-->
<!--classify each mapped probe as a false positive or true positive and calculate precision-->
<!--and recall for the pre-*de novo* and post-*de novo* VCF files from `pandora`. As-->
<!--expected, Figure {@fig:simulation_roc} shows that without *de novo* variant discovery,-->
<!--we are unable to find almost all introduced variants. In future work, we plan to add-->
<!--indels to the simulations.-->

<!--![Precision-recall curve for `pandora` simulations with (circles) and without (crosses) *de novo* variant-->
<!--discovery. This plot is measuring the proportion of SNPs we introduced into our-->
<!--simulated genome we correctly found (recall) on the X-axis. On the Y-axis is the-->
<!--proportion of calls we made which were correct (precision). Colours represent different-->
<!--coverage depth. Each point represents a different genotype confidence-->
<!--threshold.](images/simulation_roc_denovo_and_no_denovo.png){#fig:simulation_roc-->
<!--width="90%"}-->

##### Empirical data - multi-sample comparison

This section will be the major focus for the evaluation of both *de novo* *and*
`pandora` and will constitute (part of) the results section of the `pandora` paper (near
completion). We show the power of using a PRG, instead of a single reference, along with
`pandora`'s performance compared to Nanopore- and Illumina-based variant callers. We
compare 20 samples, from across the *E. coli* phylogeny. As other variant callers do not
use a PRG we carefully selected 24 single-reference genomes representing the diversity
of the *E. coli* phylogenetic tree as best as possible. Each variant caller was run once
for each reference genome with the aim being to show that the more samples in the
analysis (i.e. the larger the pan-genome of the samples), the lower the number of
variants the other callers can find.

In the process of developing an evaluation framework, we produced a python package
called `varifier` (<https://github.com/iqbal-lab-org/varifier>). Briefly, for precision,
`varifier` creates 'probes' for each variant in the VCF, using the genome the variants
were called with-respect-to. It then maps these probes to the truth genome for the
sample and determines the distance between the variant component of the probe and the
part of the truth genome it maps to. For recall evaluation, `varifier` collects all
differences in the pairwise alignment between the truth and VCF-reference genomes.
Probes are created for these differences (based on the truth genome) and they are mapped
to an augmented version of the VCF-reference genome, which has had the variants applied
to it. The mappings are then evaluated in the same way as for precision.  
As `pandora` calls variants for each sample with respect to an inferred best
approximation sequence, creating the set of truth variants is slightly different. We
perform a pairwise alignment for all pairs of samples and collect all the differences
from this alignment. We then deduplicate this panel to ensure variants are not "double
counted", meaning core genome variants would have an unbalanced effect on the overall
precision and recall. We then follow the same probe-mapping approach from `varifier`
with these truth variants.

Figure {@fig:basecall_model} shows, for a subset of four samples, two important results
regarding the effect of (unfiltered) *de novo* variant discovery in `pandora`. Firstly,
is shows that the choice of the Nanopore basecalling model has a sizeable impact - at
least for *E. coli*. When using a methylation-aware model (not default), there is a
significant increase in recall and decrease in error rate. While this has been
previously described for Enterobacteriaceae [@doi:10.1186/s13059-019-1727-y], it
highlights a weakness in the *de novo* variant discovery process. If the default model
is used (green line in Figure {@fig:basecall_model}), `pandora` has a higher error rate
if discovering variants is enabled, however, we do get an increase in recall. This means
that our discovery process is indeed picking up known systematic biases in Nanopore
reads and we will need to apply careful filters to negate this. Removal of a large
amount of this methylation-related systematic bias - by using a methylation-aware model
(orange line in Figure {@fig:basecall_model}) - shows that enabling discovery of novel
variants improves both error rate and recall for `pandora`.

![Effect of Nanopore basecalling model on `pandora` variant calling error rate (1-precision; X-axis) and recall (Y-axis). The default model is in green, whilst the methylation-aware model is in orange. The dashed line represents using the *de novo* variant discovery method in `pandora`. The line shows the effect of increasing the genotype confidence score threshold - i.e. moves line towards bottom left.](images/basecall_model.png){#fig:basecall_model
width="80%"}

For comparison with other variant callers we filtered `pandora` (Nanopore/Illumina)
variants based on the following criteria:

- Depth less than 10x/5x
- Less than 5%/5% of reads are on one strand
- 60%/80% or more of k-mers on the allele have zero coverage

As the main claim with the `pandora` method is that the use of genome graphs allows for
more power to discover variants in the accessory genome, we assess the recall in that
light. We group loci based on the number of samples they are found in and calculate the
recall for each group. The more samples a locus is found in, the more likely it is a
core genome locus. Figure {@fig:recall} shows that in loci shared by less than 13 (65%)
samples, `pandora` has recall at least inline with other variant callers for both
Nanopore and Illumina reads. However, when the number of shared samples is less than 6
(30%), `pandora`'s recall is significantly greater for both sequencing technologies.

![Variant caller recall across the pan-genome. The Y-axis shows the number of variants found. The X-axis shows, for each locus, how many samples is it found in (i.e. left is more "accessory" and right is more "core"). The panels split the results by Illumina (left) and Nanopore (right) data and variant callers. The boxes represent using different reference genomes from across the phylogenetic tree to call variants (for non-`pandor` callers).](images/recall.png){#fig:recall}

We then show in Figure {@fig:precision} that the variants called by `pandora` have
significantly higher precision than `nanopolish` and `medaka` for Nanopore data and
`samtools` for Illumina. `snippy` (Illumina) does have a superior precision to `pandora`
though. One other important point is that, aside from `snippy` all other variant
caller's precision varies quite a lot depending on the choice of reference genome.
`pandora`, on the other hand, maintains consistent precision regardless of the
phylogroup of a sample.

Both of these results highlight that the choice of reference genome can have a big
effect on the results of variant calling. However, using a genome graph to represent a
pan-genome, you gain access to variation from the accessory genome that is "invisible"
to standard linear reference-based approaches.

![Precision (Y-axis) per-sample for Illumina (left) and Nanopore (right) data. The boxes represent using different reference genomes from across the phylogenetic tree to call variants (for non-`pandora` callers) and are coloured by variant caller.](images/precision.png){#fig:precision}

Whilst I wrote nearly all of the code and associated tests for evaluating the recall for
this analysis, a lot of it has since been refactored by Leandro Ishi and by Martin Hunt
in the form of `varifier`. Due to the novelty of the method we are proposing, this has
taken quite a lot of time and thought. I wrote 1250 lines of code to evaluate the
methods, along with 3500 lines of test code to ensure there are no bugs in our
evaluation. Additionally, I built the original `snakemake` [@doi:10/gd2xzq] pipeline of
approximately 3500 lines of codes to orchestrate the entire evaluation and simulations
(However, much of this has been rewritten by Leandro Ishi).

#### Outstanding work

The major work still outstanding for this project is the direct integration of *de novo*
candidates back into the PRG. The current procedure requires a fairly laborious,
multi-step process for adding *de novo* candidates into the PRG, requiring the user to
run a separate pipeline from `pandora`. Ultimately this will need to be handled all
within the `pandora` program with no intervention from the user.  
Lastly, there is an ongoing refinement of the *de novo* variant discovery process from
the work in Chapters 2 and 3. The analysis in these chapters lean heavily on `pandora`,
but for Mtb, which has a very different pan-genome to that of *E. coli* - which was used
for most of the development of `pandora`'s methods.

<!--================================================================================-->

### Chapter 2: Applications to *M. tuberculosis* Nanopore variant calling

Public health applications for genome sequencing of Mtb generally focus on three
use-cases: species identification, prediction of drug resistance, and clustering of
samples for epidemiological purposes. In this chapter, we plan to focus on how the
methods developed in `pandora` can be used to improve clustering of samples - generally
referred to as "transmission clusters" - while, in the next chapter, we will address the
drug resistance prediction component. The intention is to be able to use Nanopore data
for public health. Therefore, this chapter will focus on a head-to-head comparison of
Nanopore and Illumina sequencing technologies for classifying transmission clusters for
Mtb. What we aim to show is that, contrary to current dogma, Nanopore sequencing
technology has advanced to the point where it can be applied to this use-case to a
standard acceptable by public health authorities.

##### Genetic clustering of samples

Although there is scientific interest in the question of identifying transmission chains
from genetic data, all the actionable public health information exists in the
identification of transmission clusters [@doi:10/d9r7; @doi:10/f8gsk2].

The first step towards clustering a set of genomes is determining a distance matrix. For
the majority of bacteria, there is a necessary step of identifying recombination tracts
\- which will contain a high density of SNPs - and removing them. Removal of these SNPs
is necessary as they will have arrived at a different rate to the putative molecular
clock and will artefactually extend branch lengths on the phylogenetic tree
[@doi:10/d9r7; @doi:10/f8gsk2]. In the case of Mtb, however, there is virtually no
recombination [@doi:10/fhqqkv; @doi:10/ftp6r2; @doi:10/f4mrqv], so this step in not
required.

<!--There are three main issues we need to address or keep in mind for the clustering-->
<!--component of this chapter:-->

<!--- When running a variant caller on a single sample, typically the tool only makes a-->
<!--  non-reference call when it finds a definite difference from the reference. Therefore,-->
<!--  it is impossible to tell the difference between a reference-matching site and one in-->
<!--  which there is genotyping uncertainty. One solution is to make a de-duplicated list of-->
<!--  variants found in all samples and genotype the samples at those positions. Solving-->
<!--  this reference-bias issue is the strength of the `compare` routine within `pandora`.-->
<!--  It tackles the problem in this manner while also handling issues of overlapping-->
<!--  variants in the list of all-sample-variants.-->
<!--- What distance measure do we use? Do we exclude positions where any sample has missing-->
<!--  data? Missing data of this type could be due to low coverage, but it could also be-->
<!--  because a section of the genome is not present. We will extensively cover this issue-->
<!--  for *E. coli* in [Chapter 1](#chapter-1-variant-discovery-in-genome-graphs). The-->
<!--  pan-genome of Mtb is small, but it does exist nonetheless [@doi:10/d9r8;-->
<!--  @doi:10/f4mrqv]. There is also a secondary issue that if certain regions have-->
<!--  clustered genetic variation in some lineages of Mtb then reads will not map well to-->
<!--  the reference. Both of the above issues are handled by `pandora` due to its use of a-->
<!--  PRG, but we still need to make a choice about missing data.-->
<!--- How do we handle sites where there is evidence of heterozygosity - i.e. a mixed-->
<!--  sample?-->

For this chapter, we define genetic distance to be the sum of genetic discordances,
where missing data and heterozygosity do not cause discordance and study the clustering
this definition generates.

#### Data

Each sample was sequenced on both Nanopore and Illumina platforms from the same isolate
and DNA extraction. In total, we received 118 samples from Madagascar, 83 from South
Africa, and 46 from the National Tuberculosis Reference Lab in Birmingham; giving us a
total of 247 samples. As these samples are not reference isolates, we need to be able to
compare both Illumina and Nanopore to a truth. To establish how each platform compares
to the truth, we have additionally sequenced 35 of the Malagasy isolates with PacBio and
will use the high-quality assemblies for validation of variant calls.

#### Quality Control

The first step in quality control (QC) was to exclude samples where the Nanopore and
Illumina data were not perfectly matched. In total, we excluded 40 samples at this
stage.

The remaining 207 samples were processed through a QC pipeline. The first step in the
pipeline is decontamination of sequencing reads. We used the decontamination database
from `clockwork` (<https://github.com/iqbal-lab-org/clockwork>), which contains a wide
range of organisms, including viral, human, Mtb, non-tuberculosis Mycobacterium (NTM),
and nasopharyngeal-associated bacterial genomes. Reads were mapped to the database and
was output to a final decontaminated fastq file if it had any mapping to a
non-contaminant genome. All decontaminated fastq files were subsampled to a depth of 60x
(Illumina) and 150x (Nanopore) using `rasusa` [@doi:10/d9rz].

The last step in the QC pipeline is to assign lineages for each sample. A panel of
lineage-defining SNPs [@doi:10/gbvbxh; @doi:10/d9r2; @doi:10/f9dg9j] was used in
conjunction with a sample's Illumina VCF from the
[Baseline variant analysis](#baseline-variant-analysis) for the lineage assignment.

In the end, we chose to exclude samples from further analysis if they met any of the
following criteria:

- Illumina coverage below 20x
- Nanopore coverage below 30x
- Evidence of mixed infection - i.e. mixed lineage classification
- Unknown lineage assignment - no valid SNPs at lineage-defining sites

This filtering criteria led to a further 57 samples being excluded; leaving us with a
total of 150 samples to use for the remainder of this work. In addition to the QC of the
Illumina/Nanopore data, we sadly had to exclude 26/35 PacBio sequencing datasets due to
mismatched Illumina/Nanopore data or PacBio coverage lower than 20x.

#### Baseline variant analysis

The truth set of variants for the Illumina data in this chapter come from running the
Public Health England pipeline, COMPASS [@doi:10/d9r9]. This pipeline will act as a
guide to inform us about whether the results from the Nanopore data are comparable with
those being used in real public health settings. COMPASS effectively uses `samtools`
[@doi:10/ff6426] to call variants and then applies a series of complex variant
filtering. As a baseline for the Nanopore data, we use `bcftools` [@doi:10/fw7k5k], with
some filtering of variants to remove low-quality calls and with a mask to avoid
repetitive and structurally variable regions of the Mtb genome.

##### Nanopore SNP concordance with Illumina

Whilst the Illumina SNP calls from COMPASS are filtered as part of the pipeline, we had
to settle on filters for the Nanopore SNP calls from `bcftools`. We used the methodology
from the section
[Comparing Illumina and Nanopore SNPs to truth assemblies](#comparing-illumina-and-nanopore-snps-to-truth-assemblies)
to refine the filters in an iterative process. In the end we filter all SNPs with
quality (QUAL column in VCF) below 60, a read position bias less than 0.05, a
segregation-based metric above -0.5, or a variant distance bias below 0.002.

To assess how well the Nanopore SNPs agree with Illumina we first look at SNP
concordance. Two metrics of interest here are the call rate - what proportion of COMPASS
alternate alleles does `bcftools` make a reference/alternate call - and the concordance
\- what proportion of COMPASS alternate alleles does `bcftools` genotype agree with.
Figure {@fig:alt_concordance} shows that concordance is very high between the two
technologies, with nearly all samples having a concordance greater than 99.5%. Call rate
is a little lower than this, with the majority of samples being above 97%.

![Call rate (Y-axis) and concordance (X-axis) of `bcftools` SNP calls to Illumina
COMPASS calls. Call rate is what proportion of COMPASS alternate alleles does
`bcftools` make a reference/alternate call. Concordance is what proportion of COMPASS
alternate alleles does `bcftools` genotype agree with. Each point represents a sample,
with samples coloured by the site the data came from.](images/alt_concordance.png){#fig:alt_concordance}

As transmission clusters are ultimately defined based on a SNP distance matrix, it is
important to understand how such matrices differ between Illumina and Nanopore variant
calls. To investigate this, a consensus sequence was generated from the filtered VCFs by
replacing reference positions with passing SNP calls. Any positions with a null genotype
or that failed the filtering were masked. Positions which do not appear in the VCF (i.e.
no reads mapped to this region) were also masked, as were positions in repetitive
regions [@doi:10/f3hxn7]. We then generate a pairwise SNP distance matrix for each
sequencing technology from their respective consensus genomes using `snp-dists`
[@doi:10/d9zj].  
Figure {@fig:dotplot} shows the relationship of these distances between pairs of samples
based on the sequencing technology used. While the relationship across all samples of
all distances is interesting, in the context of defining transmission clusters, it is
slightly misleading. Transmission clusters by grouping together samples that are within
a certain number of SNPs. The threshold used for this grouping is generally in the order
of tens-of-SNPs [@doi:10/d9r7] so it makes more sense to look at the distance
relationship for samples that are closer to each other. In Figure {@fig:close_dotplot},
we zoom in on the bottom left of Figure {@fig:dotplot}, to samples within an Illumina
SNP distance of 100. It shows that, at this scale, the relationship between Illumina-
and Nanopore-defined SNP distance is much closer. The correlation between the two can be
quantified by the linear equation $y=0.93x+0.84$, where $y$ is the predicted Nanopore
distance between two samples, given the Illumina distance $x$. We can use this equation
as a way of translating transmission cluster SNP thresholds for Illumina data to
Nanopore. For instance, if clusters are defined as samples within 12 SNPs of each other,
we can use this as $x$ and define our Nanopore transmission clusters as $y=0.93\times 12
\+ 0.84=12.0$. So at a threshold of 12 SNPs, the Nanopore threshold would be the same as
Illumina.

![Relationship between pairwise SNP distance for Illumina (COMPASS; X-axis) and Nanopore
(`bcftools`; Y-axis). Each point represents a pair of samples. The red diagonal line
is the identity line, which is where the points should lie if the distance between samples
is the same for each technology. The black line shows the line of best fit for the data.
The legend also shows the equations for these lines, along with their correlation
coefficient (r).](images/dotplot.png){#fig:dotplot}

![Relationship between pairwise SNP distance for Illumina (COMPASS; X-axis) and Nanopore
(`bcftools`; Y-axis) for samples within 100 SNPs of each other (based on Illumina distance).
Each point represents a pair of samples. The red diagonal line
is the identity line, which is where the points should lie if the distance between samples
is the same for each technology. The black line shows the line of best fit for the data.
The legend also shows the equations for these lines, along with their correlation
coefficient (r).](images/close_dotplot.png){#fig:close_dotplot}

##### Comparing Illumina and Nanopore SNPs to truth assemblies

The analyses so far have treated the Illumina SNPs as a kind of "truth". In order to get
a sense of how "correct" the SNP calls are for each technology we need to compare then
to a "truth". For the nine samples with PacBio CCS data that passed QC, we generated
assemblies (using only the CCS reads) with `flye` [@doi:10/gfzbrd]. We masked any
positions in the assembly where mapped Illumina reads did not have more than 90%
agreement with the assembly, or had less than 10 reads. One sample was excluded due to
the detection of other species contigs within the assembly. We then used `varifier` to
assess the precision and recall of the SNP calls for the eight samples with high quality
assemblies. Figure {@fig:baseline_truth} shows that ...

![Evaluation of the Illumina (COMPASS; green) and Nanopore (`bcftools`; orange) SNP calls to the PacBio CCS assemblies for eight samples using `varifier`. The left plot shows precision and the right is recall. Each point is one of the eight samples.](images/baseline_truth.png){#fig:baseline_truth}

#### Per-sample variant calls with `pandora`

The aim of this section will be to try varying degrees of PRG complexity for Mtb sample
analysis. At this stage, we have two varieties in mind:

- Sparse PRG - H37Rv and all variants from a random selection of 100 samples from each
  lineage in the CRyPTIC (Comprehensive Resistance Prediction for Tuberculosis: an
  International Consortium) dataset [@doi:10/d9kj].
- Dense PRG - The same as Sparse PRG, but 2,000 samples from each lineage.

In all of the above PRGs, we will apply the same mask from the baseline analysis and
divide the genome into genes and intergenic regions, with a local PRG for each.

For each of the PRGs, we plan to perform the following analysis. Quantify the number of
SNPs and indels `pandora` calls per-sample and see how this compares to the baseline and
truth. We will report on the concordance rate, which is the proportion of shared sites
between `pandora` and the truth that agree. Additionally, we will investigate how the
complexity of the PRG effects the call rates and what the cost in computational
performance is.

#### Multi-sample comparison

Multi-sample comparison suffers from two main challenges. First, large chunks of DNA may
be present or absent across samples - this is the pan-genome effect. This effect causes
significant issues with single-reference approaches, as outlined previously, but
`pandora` was developed to address this. Second, when comparing a set of samples, the
choice of reference affects how one describes variants. `pandora`, by design, chooses a
reference for each loci PRG based on the current dataset, intending to maximise the
succinctness of variant descriptions (see Figure {@fig:refbias}). For example, we want
to see SNPs as what they are - single-base variants - not as a nested region (as in
Figure {@fig:refbias}).

As mentioned, we handle these cases implicitly in `pandora`, however, the aim of this
chapter and the next, in addition to comparing across technologies, will also be to
compare methods for gaining drug resistance and epidemiological clustering information.
To be able to compare with other methods though, we will need to be able to compare
variants called with respect to a single-reference by other tools, with those from
`pandora`.

With that in mind, this section will focus on producing a distance matrix for all
samples using the result of the `pandora compare` routine and contrast this to those
obtained from the single-reference methods.

#### Reproducing "truth" Illumina transmission clusters

In this section we will examine how well we can recreate the transmission clusters
produced from COMPASS SNP calls with Nanopore data. Ultimately we will conclude with a
recommendation of which method to call variants with: `bcftools`, `pandora map`
(single-sample), or `pandora compare` (multi-sample). And what SNP threshold is required
to ensure clusters are as similar as possible - if it *is* possible to get comparable
clusters.

---

All of the analysis in this chapter was performed by myself - with the exception of the
running of COMPASS - and all of the pipelines can be found at
<https://github.com/mbhall88/head_to_head_pipeline>.

#### Outstanding work

I am currently producing the `pandora` variant calls. This has required an iterative
refinement of the *de novo* variant discovery process - thus improving it's ability to
detect clustered variants. Additionally, there is likely to be some software development
time required to allow the `compare` routine to deal with samples of varying coverage.
This will entail refining some prototyping that has been done already on genotype
confidence percentile, which allows for normalising over coverage.  
Once the `pandora` variants are in-hand, there just remains the clustering of samples.
We don't anticipate any impediments to generating clusters and it *should* be a fairly
quick process.

<!--================================================================================-->

### Chapter 3: Applications to improving *M. tuberculosis* drug resistance prediction

The genetic basis for drug resistance in Mtb is only partially understood. For the four
first-line drugs, it is possible to detect the majority of resistant strains with high
confidence [@doi:10/d9kj], but for second-line, novel, and repurposed drugs, this is
much harder.

Previous work from our group has shown that using whole-genome sequencing (WGS) it is
possible to create a panel of resistance markers and then successfully use this panel to
predict drug resistance from Mtb sequence data [@doi:10/f755tg; @doi:10/f3jjtq]. This
work involved the development of a software program called `mykrobe` to automate this
inference and can use either Illumina or Nanopore data [@doi:10/f94vt4]. During the
previous year, the predictive power of `mykrobe` has expanded and become even more
accurate [@doi:10/ggd835] (I played a small role in improving the likelihood
calculations). This update of the `mykrobe` panel was mostly due to the recently
published work from the CRyPTIC project [@doi:10/d9kj]. CRyPTIC aims to perform drug
susceptibility testing and WGS on 40,000 Mtb samples (many MDR) from all over the globe,
and combine this with WGS data from another 60,000 samples. The goal of the project is
to improve genotypic resistance prediction by expanding our catalogue of resistance
mutations.

The proposed work for this chapter is based on the assumption that a large part of the
work by this consortium (which our group is a critical part of) will be available. While
there has already been a significant amount of data produced from CRyPTIC, there will be
more coming during the remainder of my PhD. Given the collection of SNPs and indels
identified as being necessary for resistance to the 14 major drugs tested, we want to
show that we can detect them as well with Nanopore data as we can with Illumina.

#### Limitations of existing methods

Although there are many tools available for predicting drug resistance in Mtb from
Illumina WGS, only two support the use of Nanopore data - `mykrobe` and `tb-profiler`
[@doi:10/d949]. In the recent update to `mykrobe` from our group [@doi:10/ggd835], we
have shown that the primary factor determining how well a tool performed was the
catalogue of resistance mutations used. However, given the same panel, different tools
do not perform identically, and therefore methodology is still important. While some
studies have focused specifically on Nanopore data, all used a small sample size
(Votintseva *et al.* 2017 n=5, Hunt *et al.* 2019 n=5, `tb-profiler` 2019 n=3).
Additionally, `tb-profiler` and `mykrobe` both have limitations with their methods.

`tb-profiler` uses an approach similar to a read pileup and then taking consensus at
each site. This means they will never perform particularly well with indel calls as
pileups do not work well for Illumina [@doi:10/d95c], let alone Nanopore. Given that
indel calls are critical for some drugs in Mtb such as pyrazinamide [@doi:10/d9kj], this
becomes a ceiling for the `tb-profiler` method. On the other hand, `mykrobe` uses a
conservative, precise approach of mapping 21-mers to a de Bruijn graph. With an
approximate error rate of 10% in Nanopore data, the probability of a correct 21-mer is
11%. Meaning that `mykrobe` requires high depth-of-coverage to get sufficient, correct
21-mers. By comparison, `pandora` maps entire reads to genes - rather than k-mers in
isolation as with `mykrobe` - confirms that the read really does overlap the gene, and
only then includes the k-mers in the calculation. Theoretically, this allows for the use
of a smaller k-mer size and therefore reduces coverage requirements.  
A second limitation of both `tb-profiler` and `mykrobe` is that they only genotype with
respect to know variants - i.e. they cannot detect novel variants. The CRyPTIC
consortium recently introduced a new approach whereby if an unknown mutation is
identified in a gene known to be involved in resistance, they refuse to make a call and
instead send the sample for phenotyping [@doi:10/d9kj]. On their 10,000 samples, this
achieved a specificity and sensitivity for first-line drugs that was acceptable for
clinical usage. This method is now in use at Public Health England for all Mtb samples
in England. Hunt *et al.* 2019 [@doi:10/ggd835] quantified the cost of the
pure-genotyping approach of `mykrobe`, showing that 2.4-4.6% of resistant samples were
missed. By introducing *de novo* discovery into `pandora`, I enable us to address this
issue, and that is the focus for this chapter.

#### Drug susceptibility prediction for *M. tuberculosis* using `pandora`

The work in this chapter will aim to predict drug-resistance for Mtb using `pandora` and
its new *de novo* component introduced in
[Chapter 1](#chapter-1-variant-discovery-in-genome-graphs). The first step in this will
be producing a gene-succinct PRG that includes variants from the above-mentioned CRyPTIC
work that are known to cause resistance or susceptibility. This PRG will be easy to
build as the alleles and probes for these variants-of-interest are already defined for
`mykrobe`. I will write a software program (either an extension of `pandora` or a
standalone tool) that takes the output of `pandora` used with this PRG and makes
predictions about resistance, susceptibility, and whether phenotyping should be
performed. As we know whether an allele causes resistance or susceptibility, this
prediction will be straightforward to implement.

I plan to validate this approach with the data from
[Chapter 2](#chapter-2-applications-to-m-tuberculosis-nanopore-variant-calling),
comparing concordance with `mykrobe` for Illumina and Nanopore. In particular, the
analysis will focus on the (hopefully) lower coverage required by `pandora` to achieve
the same, or better, results as `mykrobe`, and the increased detection power provided by
*de novo* variant discovery.

### Chapter 4: Construction of a *M. tuberculosis* reference pan-genome

Although Mtb has a closed pan-genome due to its lack of recombination and horizontal
gene transfer [@doi:10/fhqqkv; @doi:10/ftp6r2; @doi:10/f4mrqv], there are reasons why a
pan-genome would be useful to the community. First of all, some genes exist within the
pan-genome that are not present in the H37Rv reference genome [@doi:10/gchzjz].
Secondly, approximately 10% of the genome consists of so-called *pe/ppe* genes. These
genes have a high GC-content, are very repetitive, and have been implicated in immune
evasion and virulence [@doi:10/gbpvsd; @doi:10/f8sf3h]. The *pe/ppe* genes also harbour
a disproportionately large amount of genetic diversity between isolates and a nucleotide
diversity approximately 2-fold higher than the rest of the genome [@doi:10/f8sf3h]. They
are sufficiently similar that short reads fail to map, and are frequently masked out of
the genome for variant calling. The ability to accurately map sequencing reads to these
genes would likely improve our ability to perform variant calling in Mtb and therefore
better determine how isolates relate to each other.

For this chapter, the aim is to build a high-quality pan-genome for Mtb, to allow
variant discovery in *all* genes - ideally including the *pe/ppe* genes.

#### Assembly and multiple sequence alignment of high-quality *M. tuberculosis* genomes

To facilitate the desired outcomes of this chapter, we will first assemble the highest
quality Mtb genomes from
[Chapter 2](#chapter-2-applications-to-m-tuberculosis-nanopore-variant-calling). These
genomes, in the worst-case, have matched Illumina and Nanopore data and, in the
best-case, PacBio too. The idea is that these assemblies will serve as the scaffold for
the Mtb pan-genome, with the addition of other high-quality genomes outside of this
thesis (2 PacBio assemblies from every lineage), and population variants discovered from
the CRyPTIC consortium.

After assembly, we plan to perform large-scale multiple sequence alignment of these
genomes to investigate how stable the Mtb genome is when ignoring the *pe/ppe* genes.
The unknown quantity going into this chapter will be how easily *pe/ppe* genes can be
assigned and matched across genomes. It may end up being necessary to assign genes from
these genomes using synteny and parsimony with existing gene identifiers.

Ultimately, the overall gene ordering is not of great importance for the construction of
a `pandora`-friendly pan-genome. The design of `pandora` is such that we divide the
pan-genome into discrete pieces (loci) - i.e. genes and intergenic regions. Due to the
low nucleotide diversity of Mtb, it may end up being necessary to split the Mtb genome
into synteny blocks rather than the standard gene/intergenic approach we have used for
other bacteria.

#### A genome graph map of *pe/ppe* genes

One question which will be of particular interest for this section will be whether reads
covering one *pe/ppe* gene map to various others. If, as shown by others, *pe/ppe* genes
arose through gene conversion [@pmc:PMC1660551], we would expect this to be the case.
However, having high-quality assemblies built from a combination of long- and short-read
technologies, we hope we can improve on the current nucleotide resolution and allow more
accurate mapping to these genes. The main deliverable from this section will be a
collection of high-quality *pe/ppe* PRGs with information about what read length will
provide reliable mapping, and whether Illumina data can be reliably mapped to them. For
some genes, this may be a no, but for others, we expect it will be possible to reliably
map shorter reads than before.

#### Re-analysis of head-to-head data

Using this newly constructed, high-quality pan-genome, without the *pe/ppe* genes
masked, we will re-analyse some of the data from the head-to-head analysis in [Chapter
2](#chapter-2-applications-to-m-tuberculosis-nanopore-variant-calling). The aim is to
see how many more variants we find, and whether we are better able to cluster samples as
a result of having access to high-quality *pe/ppe* PRGs.

#### *pe/ppe* genetic variation in 10000 genomes

As mentioned previously, the CRyPTIC consortium are sequencing tens of thousands of Mtb
genomes. In this section, we will look at the *pe/ppe* variation across 10,000 Mtb
genomes using our newly constructed Mtb pan-genome and present the patterns we find.
This work will be aided by a mycobacteriologist postdoc in our group who has extensive
knowledge of Mtb biology.

